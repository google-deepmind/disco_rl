{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I837hfXK8CAd"
      },
      "source": [
        "**NOTE:** When running in public colab, this cell will prompt you to upload the archive with the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkPWk7TR7922"
      },
      "outputs": [],
      "source": [
        "# @title Environment setup\n",
        "\n",
        "\n",
        "!pip install git+https://github.com/google-deepmind/disco_rl.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_QQRMPQaLip"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "import chex\n",
        "import distrax\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from matplotlib import pyplot as plt\n",
        "from ml_collections import config_dict\n",
        "import numpy as np\n",
        "import optax\n",
        "import rlax\n",
        "import tqdm\n",
        "\n",
        "loaded_update_rule_params = None\n",
        "rng_key = jax.random.PRNGKey(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pmTtaU8AX6m"
      },
      "outputs": [],
      "source": [
        "# Types \u0026 utils\n",
        "from disco_rl import types\n",
        "from disco_rl import utils\n",
        "\n",
        "# Environments\n",
        "from disco_rl.environments import base as base_env\n",
        "from disco_rl.environments import jittable_envs\n",
        "\n",
        "# Learning\n",
        "from disco_rl import agent as agent_lib\n",
        "from disco_rl.value_fns import value_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InYv8QC4AX6m"
      },
      "outputs": [],
      "source": [
        "eta_path = ''\n",
        "if loaded_update_rule_params is None:\n",
        "  loaded_update_rule_params = {}\n",
        "  with open(eta_path, 'rb') as file:\n",
        "    ur_params_wb = dict(np.load(file))\n",
        "    for key_wb in ur_params_wb:\n",
        "      key = key_wb[:-2]\n",
        "      loaded_update_rule_params[key] = {\n",
        "          'b': ur_params_wb[f'{key}/b'],\n",
        "          'w': ur_params_wb[f'{key}/w'],\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhHP6mbdE9Rx"
      },
      "outputs": [],
      "source": [
        "agent_settings = agent_lib.get_settings_disco()\n",
        "axis_name = 'i'\n",
        "\n",
        "\n",
        "def get_env(batch_size):\n",
        "  return jittable_envs.CatchJittableEnvironment(\n",
        "      batch_size=batch_size,\n",
        "      env_settings=config_dict.ConfigDict(dict(rows=5, columns=5)),\n",
        "  )\n",
        "\n",
        "\n",
        "env = get_env(batch_size=1)\n",
        "\n",
        "agent_settings.net_settings.name = 'mlp'\n",
        "agent_settings.net_settings.net_args = dict(\n",
        "    dense=(512, 512),\n",
        "    model_arch_name='lstm',\n",
        "    head_w_init_std=1e-2,\n",
        "    model_kwargs=dict(\n",
        "        head_mlp_hiddens=(256,),\n",
        "        lstm_size=256,\n",
        "    ),\n",
        ")\n",
        "agent_settings.learning_rate = 5e-4\n",
        "agent_settings.end_learning_rate = agent_settings.learning_rate\n",
        "\n",
        "agent = agent_lib.Agent(\n",
        "    agent_settings=agent_settings,\n",
        "    single_observation_spec=env.single_observation_spec(),\n",
        "    single_action_spec=env.single_action_spec(),\n",
        "    batch_axis_name=axis_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWWHy11qTaCT"
      },
      "outputs": [],
      "source": [
        "random_update_rule_params, _ = agent.update_rule.init_params(rng_key)\n",
        "\n",
        "if agent_settings.update_rule_name == 'disco':\n",
        "  chex.assert_trees_all_equal_shapes_and_dtypes(\n",
        "      random_update_rule_params, loaded_update_rule_params\n",
        "  )\n",
        "else:\n",
        "  print('Not using a discovered rule.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FZcXv5kvlWW"
      },
      "outputs": [],
      "source": [
        "# Configs\n",
        "num_agents = 1\n",
        "rollout_len = 16\n",
        "num_inner_steps = 1\n",
        "batch_size_per_device = 128\n",
        "rng_key = jax.random.PRNGKey(12)\n",
        "\n",
        "update_rule_params = random_update_rule_params  # loaded_update_rule_params\n",
        "\n",
        "value_fn_config = types.ValueFnConfig(\n",
        "    net='mlp',\n",
        "    net_args=dict(\n",
        "        dense=(256, 256),\n",
        "        head_w_init_std=1e-2,\n",
        "        action_spec=(),\n",
        "    ),\n",
        "    learning_rate=1e-3,\n",
        "    max_abs_update=1.0,\n",
        "    discount_factor=0.99,\n",
        "    td_lambda=0.96,\n",
        "    outer_value_cost=1.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbQX_ifgxogZ"
      },
      "outputs": [],
      "source": [
        "def unroll_jittable_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "):\n",
        "\n",
        "  def _single_step(carry, step_rng):\n",
        "    env_state, ts, actor_state = carry\n",
        "    actor_timestep, actor_state = actor_step_fn(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    env_state, ts = env.step(env_state, actor_timestep.actions)\n",
        "    return (env_state, ts, actor_state), actor_timestep\n",
        "\n",
        "  (env_state, ts, actor_state), actor_rollout = jax.lax.scan(\n",
        "      _single_step,\n",
        "      (env_state, ts, actor_state),\n",
        "      jax.random.split(rng, rollout_len),\n",
        "  )\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(actor_rollout)\n",
        "  return actor_rollout, actor_state, ts, env_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8ZHIAzZgneH"
      },
      "outputs": [],
      "source": [
        "@chex.dataclass\n",
        "class MetaTrainState:\n",
        "  learner_state: agent_lib.LearnerState\n",
        "  actor_state: types.HaikuState\n",
        "  value_state: types.ValueState\n",
        "  env_state: Any\n",
        "  env_timestep: types.EnvironmentTimestep\n",
        "\n",
        "\n",
        "class MetaTrainAgent:\n",
        "  agent: agent_lib.Agent\n",
        "  value_fn: value_fn.ValueFunction\n",
        "  env: base_env.Environment\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      batch_size_per_device: int,\n",
        "      agent_settings: config_dict.ConfigDict,\n",
        "      value_fn_config: types.ValueFnConfig,\n",
        "      axis_name: str | None = None,\n",
        "  ):\n",
        "    self.env = get_env(batch_size_per_device)\n",
        "    self.agent = agent_lib.Agent(\n",
        "        agent_settings=agent_settings,\n",
        "        single_observation_spec=self.env.single_observation_spec(),\n",
        "        single_action_spec=self.env.single_action_spec(),\n",
        "        batch_axis_name=axis_name,\n",
        "    )\n",
        "    self.value_fn = value_fn.ValueFunction(value_fn_config, axis_name=axis_name)\n",
        "    self._env = env\n",
        "\n",
        "    self._unroll_jittable_actor = jax.jit(\n",
        "        unroll_jittable_actor,\n",
        "        static_argnames=('env', 'rollout_len', 'actor_step_fn'),\n",
        "    )\n",
        "\n",
        "  def init_state(self, rng_key: chex.PRNGKey) -\u003e MetaTrainState:\n",
        "    dummy_obs = utils.zeros_like_spec(\n",
        "        self.env.single_observation_spec(),\n",
        "        prepend_shape=(batch_size_per_device,),\n",
        "    )\n",
        "    rng_keys = jax.random.split(rng_key, 3)\n",
        "    env_state, env_timestep = self.env.reset(rng_key)\n",
        "    return MetaTrainState(\n",
        "        learner_state=self.agent.initial_learner_state(rng_keys[0]),\n",
        "        actor_state=self.agent.initial_actor_state(rng_keys[1]),\n",
        "        value_state=self.value_fn.initial_state(rng_keys[2], dummy_obs),\n",
        "        env_state=env_state,\n",
        "        env_timestep=env_timestep,\n",
        "    )\n",
        "\n",
        "  @property\n",
        "  def learner_step(self):\n",
        "    return self.agent.learner_step\n",
        "\n",
        "  @property\n",
        "  def actor_step(self):\n",
        "    return self.agent.actor_step\n",
        "\n",
        "  @property\n",
        "  def unroll_net(self):\n",
        "    return self.agent.unroll_net\n",
        "\n",
        "  def unroll_actor(\n",
        "      self, state: MetaTrainState, rng: chex.PRNGKey, rollout_len: int\n",
        "  ) -\u003e tuple[MetaTrainState, types.ActorRollout]:\n",
        "    params = state.learner_state.params\n",
        "    actor_state = state.actor_state\n",
        "    ts = state.env_timestep\n",
        "    env_state = state.env_state\n",
        "    rollout, actor_state, env_timestep, env_state = self._unroll_jittable_actor(\n",
        "        params,\n",
        "        actor_state,\n",
        "        ts,\n",
        "        env_state,\n",
        "        rng,\n",
        "        self.env,\n",
        "        rollout_len,\n",
        "        self.agent.actor_step,\n",
        "    )\n",
        "    new_state = MetaTrainState(\n",
        "        learner_state=state.learner_state,\n",
        "        actor_state=actor_state,\n",
        "        value_state=state.value_state,\n",
        "        env_state=env_state,\n",
        "        env_timestep=env_timestep,\n",
        "    )\n",
        "    return new_state, rollout\n",
        "\n",
        "\n",
        "# Create multiple agents.\n",
        "agents = []\n",
        "agents_states = []\n",
        "rng, rng_key = jax.random.split(rng_key)\n",
        "for rng in jax.random.split(rng, num_agents):\n",
        "  agents.append(\n",
        "      MetaTrainAgent(\n",
        "          batch_size_per_device=batch_size_per_device,\n",
        "          agent_settings=agent_settings,\n",
        "          value_fn_config=value_fn_config,\n",
        "      )\n",
        "  )\n",
        "  agents_states.append(agents[-1].init_state(rng))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amRhz7rAAeYO"
      },
      "outputs": [],
      "source": [
        "def outer_grad(\n",
        "    update_rule_params: types.MetaParams,\n",
        "    agent_state: MetaTrainState,\n",
        "    train_rollouts: types.ActorRollout,\n",
        "    valid_rollout: types.ActorRollout,\n",
        "    rng: chex.PRNGKey,\n",
        "    agent: MetaTrainAgent,\n",
        "    axis_name: str | None = axis_name,\n",
        "):\n",
        "\n",
        "  unroll_len = train_rollouts.rewards.shape[0]\n",
        "\n",
        "  def _inner_step(carry, inputs):\n",
        "    update_rule_params, learner_state, actor_state, value_state = carry\n",
        "    actor_rollout, learner_rng = inputs\n",
        "\n",
        "    # Update learner.\n",
        "    new_learner_state, new_actor_state, metrics = agent.learner_step(\n",
        "        rng=learner_rng,\n",
        "        rollout=actor_rollout,\n",
        "        learner_state=learner_state,\n",
        "        agent_net_state=actor_state,\n",
        "        update_rule_params=update_rule_params,\n",
        "    )\n",
        "\n",
        "    # Update value function.\n",
        "    agent_out, _ = agent.unroll_net(\n",
        "        learner_state.params, actor_state, actor_rollout\n",
        "    )\n",
        "    new_value_state, _, _ = agent.value_fn.update(\n",
        "        value_state, actor_rollout, agent_out['logits']\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        update_rule_params,\n",
        "        new_learner_state,\n",
        "        new_actor_state,\n",
        "        new_value_state,\n",
        "    ), metrics\n",
        "\n",
        "  def _outer_loss(\n",
        "      update_rule_params: types.MetaParams,\n",
        "      agent_state: MetaTrainState,\n",
        "      train_rollouts: types.ActorRollout,\n",
        "      valid_rollout: types.ActorRollout,\n",
        "  ):\n",
        "    \"\"\"Calculates loss for the update rule.\"\"\"\n",
        "    train_rng, valid_rng = jax.random.split(rng, 2)\n",
        "\n",
        "    # Perform inner steps (i.e. updates).\n",
        "    learner_rngs = jax.random.split(train_rng, unroll_len)\n",
        "    (_, new_learner_state, new_actor_state, new_value_state), train_metrics = (\n",
        "        jax.lax.scan(\n",
        "            _inner_step,\n",
        "            (\n",
        "                update_rule_params,\n",
        "                agent_state.learner_state,\n",
        "                agent_state.actor_state,\n",
        "                agent_state.value_state,\n",
        "            ),\n",
        "            (train_rollouts, learner_rngs),\n",
        "        )\n",
        "    )\n",
        "    train_meta_out = train_metrics.pop('meta_out')\n",
        "\n",
        "    # Run inference on the validation rollout.\n",
        "    agent_rollout_on_valid, _ = hk.BatchApply(\n",
        "        lambda ts: agent.actor_step(\n",
        "            actor_params=new_learner_state.params,\n",
        "            rng=valid_rng,\n",
        "            timestep=ts,\n",
        "            actor_state=valid_rollout.first_state(time_axis=0),\n",
        "        )\n",
        "    )(valid_rollout.to_env_timestep())\n",
        "\n",
        "    # Calculate value_fn on the validation rollout.\n",
        "    value_out, _, _, _ = agent.value_fn.get_value_outs(\n",
        "        new_value_state, valid_rollout, agent_rollout_on_valid['logits']\n",
        "    )\n",
        "\n",
        "    actions_on_valid = valid_rollout.actions[:-1]\n",
        "    logits_on_valid = agent_rollout_on_valid['logits'][:-1]\n",
        "    adv_t = jax.lax.stop_gradient(value_out.normalized_adv)\n",
        "    pg_loss_per_step = utils.differentiable_policy_gradient_loss(\n",
        "        logits_on_valid, actions_on_valid, adv_t=adv_t, backprop=False\n",
        "    )\n",
        "    entropy_loss_per_step = -distrax.Softmax(logits_on_valid).entropy()\n",
        "\n",
        "    # Compute policy gradient loss.\n",
        "    chex.assert_rank((pg_loss_per_step, entropy_loss_per_step), 2)  # [T, B]\n",
        "    rl_loss = (pg_loss_per_step + 1e-2 * entropy_loss_per_step).mean()\n",
        "\n",
        "    # Meta regularizers.\n",
        "    reg_loss = 0\n",
        "\n",
        "    # Validation regularisers.\n",
        "    agent_out_on_valid = agent_rollout_on_valid.agent_outs\n",
        "    z_a = utils.batch_lookup(agent_out_on_valid['z'][:-1], actions_on_valid)\n",
        "    y_entropy_loss = -jnp.mean(\n",
        "        distrax.Softmax(agent_out_on_valid['y']).entropy()\n",
        "    )\n",
        "    z_entropy_loss = -jnp.mean(distrax.Softmax(z_a).entropy())\n",
        "    reg_loss += 1e-3 * (y_entropy_loss + z_entropy_loss)\n",
        "\n",
        "    # Train regularisers.\n",
        "    dp, dy, dz = train_meta_out['pi'], train_meta_out['y'], train_meta_out['z']\n",
        "    chex.assert_equal_shape_prefix([dp, dy, dz], 3)  # [N, T, B, ...]\n",
        "    reg_loss += 1e-3 * jnp.mean(jnp.square(jnp.mean(dy, axis=(1, 2, 3))))\n",
        "    reg_loss += 1e-3 * jnp.mean(jnp.square(jnp.mean(dz, axis=(1, 2, 3))))\n",
        "    reg_loss += 1e-3 * jnp.mean(jnp.square(jnp.mean(dp, axis=(1, 2, 3))))\n",
        "    logits = train_meta_out['target_out']['logits'][:, :-1]\n",
        "    chex.assert_equal_shape([logits, dp])  # [N, T, B, A]\n",
        "    target_kl_loss = rlax.categorical_kl_divergence(\n",
        "        jax.lax.stop_gradient(logits), dp\n",
        "    )\n",
        "    reg_loss += 1e-2 * jnp.mean(target_kl_loss)\n",
        "\n",
        "    # Meta loss.\n",
        "    meta_loss = rl_loss.mean() + reg_loss\n",
        "\n",
        "    meta_log = dict(\n",
        "        adv=value_out.adv.mean(),\n",
        "        normalized_adv=value_out.normalized_adv.mean(),\n",
        "        entropy=distrax.Softmax(logits_on_valid).entropy().mean(),\n",
        "        value=value_out.value.mean(),\n",
        "        val_importance_weight=jnp.mean(jnp.minimum(value_out.rho, 1.0)),\n",
        "        meta_loss=meta_loss,\n",
        "        rl_loss=rl_loss,\n",
        "        reg_loss=reg_loss,\n",
        "    )\n",
        "    new_agent_state = MetaTrainState(\n",
        "        learner_state=new_learner_state,\n",
        "        actor_state=new_actor_state,\n",
        "        value_state=new_value_state,\n",
        "        env_state=agent_state.env_state,\n",
        "        env_timestep=agent_state.env_timestep,\n",
        "    )\n",
        "\n",
        "    return meta_loss, (new_agent_state, train_metrics, meta_log)\n",
        "\n",
        "  meta_grads, outputs = jax.grad(_outer_loss, has_aux=True)(\n",
        "      update_rule_params, agent_state, train_rollouts, valid_rollout\n",
        "  )\n",
        "  new_agent_state, train_metrics, meta_log = outputs\n",
        "  if axis_name is not None:\n",
        "    (meta_grads, train_metrics, meta_log) = jax.lax.pmean(\n",
        "        (meta_grads, train_metrics, meta_log), axis_name\n",
        "    )\n",
        "\n",
        "  return meta_grads, (new_agent_state, train_metrics, meta_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLagkSk38Skl"
      },
      "outputs": [],
      "source": [
        "def outer_update(\n",
        "    update_rule_params: types.MetaParams,\n",
        "    meta_opt_state: optax.OptState,\n",
        "    agents_states: list[MetaTrainState],\n",
        "    rng: chex.PRNGKey,\n",
        "    axis_name: str | None = axis_name,\n",
        "):\n",
        "\n",
        "  # Generate inputs.\n",
        "  train_rollouts = [None] * num_agents\n",
        "  valid_rollouts = [None] * num_agents\n",
        "  rng_act, rng_upd = jax.random.split(rng)\n",
        "  rngs_per_agent_act = jax.random.split(rng_act, num_agents)\n",
        "  for agent_i in range(num_agents):\n",
        "    a, state = agents[agent_i], agents_states[agent_i]\n",
        "    rollouts = [None] * num_inner_steps\n",
        "    rngs_per_step = jax.random.split(\n",
        "        rngs_per_agent_act[agent_i], num_inner_steps\n",
        "    )\n",
        "    for step_i in range(num_inner_steps):\n",
        "      state, rollouts[step_i] = a.unroll_actor(\n",
        "          state, rngs_per_step[step_i], rollout_len\n",
        "      )\n",
        "    train_rollouts[agent_i] = utils.tree_stack(rollouts)\n",
        "    agents_states[agent_i], valid_rollouts[agent_i] = a.unroll_actor(\n",
        "        state, rngs_per_agent_act[agent_i], rollout_len * 2\n",
        "    )\n",
        "\n",
        "  # Calculate meta gradients.\n",
        "  meta_grads = [None] * num_agents\n",
        "  rngs_per_agent_upd = jax.random.split(rng_upd, num_agents)\n",
        "  metrics, meta_log = None, None\n",
        "  for agent_i in range(num_agents):\n",
        "    meta_grads[agent_i], (agents_states[agent_i], metrics, meta_log) = (\n",
        "        outer_grad(\n",
        "            update_rule_params=update_rule_params,\n",
        "            agent_state=agents_states[agent_i],\n",
        "            train_rollouts=train_rollouts[agent_i],\n",
        "            valid_rollout=valid_rollouts[agent_i],\n",
        "            rng=rngs_per_agent_upd[agent_i],\n",
        "            agent=agents[agent_i],\n",
        "            axis_name=axis_name,\n",
        "        )\n",
        "    )\n",
        "\n",
        "  # Log rewards and proportion positive rewards.\n",
        "  rewards = [None] * num_agents\n",
        "  pos_rewards = [None] * num_agents\n",
        "  neg_rewards = [None] * num_agents\n",
        "\n",
        "  for agent_i in range(num_agents):\n",
        "    assert train_rollouts[agent_i] is not None\n",
        "    r = train_rollouts[agent_i].rewards\n",
        "    rewards[agent_i] = r.mean()\n",
        "\n",
        "    pos_rewards[agent_i] = (r \u003e 0).sum()\n",
        "    neg_rewards[agent_i] = (r \u003c 0).sum()\n",
        "\n",
        "  # Pass through meta optimizer.\n",
        "  meta_gradient = jax.tree.map(\n",
        "      lambda x: x.mean(axis=0), utils.tree_stack(meta_grads)\n",
        "  )\n",
        "  meta_update, meta_opt_state = meta_opt.update(meta_gradient, meta_opt_state)\n",
        "  update_rule_params = optax.apply_updates(update_rule_params, meta_update)\n",
        "\n",
        "  meta_log['meta_grad_norm'] = optax.global_norm(meta_gradient)\n",
        "  meta_log['meta_up_norm'] = optax.global_norm(meta_update)\n",
        "  meta_log['rewards'] = utils.tree_stack(rewards).mean()\n",
        "  meta_log['pos_rewards'] = utils.tree_stack(pos_rewards).mean()\n",
        "  meta_log['neg_rewards'] = utils.tree_stack(neg_rewards).mean()\n",
        "\n",
        "  return update_rule_params, meta_opt_state, agents_states, metrics, meta_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA6VT-jaaJCf"
      },
      "outputs": [],
      "source": [
        "devices = jax.devices()\n",
        "if axis_name is not None:\n",
        "  jit_outer_update = jax.pmap(\n",
        "      outer_update, axis_name=axis_name, devices=devices\n",
        "  )\n",
        "else:\n",
        "  jit_outer_update = jax.jit(outer_update)\n",
        "\n",
        "num_steps = 1000\n",
        "(rng,) = jax.random.split(rng_key, 1)\n",
        "\n",
        "meta_opt = optax.adam(5e-4)\n",
        "meta_opt_state = meta_opt.init(update_rule_params)\n",
        "\n",
        "meta_log = {}\n",
        "metrics = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrZ3SwBS7_gG"
      },
      "outputs": [],
      "source": [
        "# Run meta-training (note that compilation can take time!).\n",
        "\n",
        "if axis_name is None:\n",
        "  step_update_rule_params = update_rule_params\n",
        "  step_meta_opt_state = meta_opt_state\n",
        "  step_agents_states = agents_states\n",
        "else:\n",
        "  step_update_rule_params = jax.device_put_replicated(\n",
        "      update_rule_params, devices\n",
        "  )\n",
        "  step_meta_opt_state = jax.device_put_replicated(meta_opt_state, devices)\n",
        "  step_agents_states = jax.device_put_replicated(agents_states, devices)\n",
        "\n",
        "for meta_step in tqdm.tqdm(range(num_steps)):\n",
        "  if meta_step in metrics:  # to support interrupting and continuing\n",
        "    continue\n",
        "\n",
        "  rng, step_rngs = jax.random.split(rng)\n",
        "  if axis_name is not None:\n",
        "    step_rngs = jax.random.split(step_rngs, len(devices))\n",
        "\n",
        "  (\n",
        "      step_update_rule_params,\n",
        "      step_meta_opt_state,\n",
        "      step_agents_states,\n",
        "      metrics[meta_step],\n",
        "      meta_log[meta_step],\n",
        "  ) = jit_outer_update(\n",
        "      update_rule_params=step_update_rule_params,\n",
        "      meta_opt_state=step_meta_opt_state,\n",
        "      agents_states=step_agents_states,\n",
        "      rng=step_rngs,\n",
        "  )\n",
        "  metrics[meta_step], meta_log[meta_step] = jax.device_get(\n",
        "      (metrics[meta_step], meta_log[meta_step])\n",
        "  )\n",
        "\n",
        "if axis_name is not None:\n",
        "  metrics, meta_log = utils.gather_from_devices((metrics, meta_log))\n",
        "  metrics, meta_log = jax.tree.map(lambda x: x.mean(0), (metrics, meta_log))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erzgtMrjNP3U"
      },
      "outputs": [],
      "source": [
        "meta_log_cpu = jax.device_get(meta_log)\n",
        "steps = np.sort(np.unique(list(meta_log_cpu.keys())))\n",
        "rows = []\n",
        "for i in steps:\n",
        "  for key in (\n",
        "      'meta_grad_norm',\n",
        "      'meta_up_norm',\n",
        "      'meta_loss',\n",
        "      'reward',\n",
        "      'pos_rewards',\n",
        "      'neg_rewards',\n",
        "  ):\n",
        "    rows.append(dict(step=i, value=float(meta_log_cpu[i][key]), f=key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU9x4cEyLpyT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "sns.relplot(\n",
        "    data=df[df.f.isin(['pos_rewards', 'neg_rewards'])],\n",
        "    x='step',\n",
        "    y='value',\n",
        "    kind='line',\n",
        "    hue='f',\n",
        "    errorbar=None,\n",
        "    aspect=1.5,\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTTOZwI5E5KF"
      },
      "outputs": [],
      "source": [
        "sns.relplot(\n",
        "    data=df[\n",
        "        df.f.isin(['meta_grad_norm', 'meta_loss', 'meta_up_norm', 'rewards'])\n",
        "    ],\n",
        "    x='step',\n",
        "    y='value',\n",
        "    kind='line',\n",
        "    col='f',\n",
        "    errorbar=None,\n",
        "    aspect=1.5,\n",
        "    facet_kws={'sharey': False, 'sharex': True},\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7Rt79q3-2pI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
