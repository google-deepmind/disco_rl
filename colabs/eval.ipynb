{
  "cells": [
    {
      "metadata": {
        "id": "xDyUmhfopb4S"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluating discovered update rules\n",
        "\n",
        "This colab demonstrates how to instantiate the `Disco103` update rule and use it for training an RL agent on a jittable version of `Catch`.\n",
        "\n",
        "The repository also contains `ActorCritic` and `PolicyGradient` update rules and a CPU version of `Catch`; feel free to explore and repurpose this code for your needs."
      ]
    },
    {
      "metadata": {
        "id": "Q_QQRMPQaLip"
      },
      "cell_type": "code",
      "source": [
        "# @title Install the package.\n",
        "\n",
        "!pip install git+https://github.com/google-deepmind/disco_rl.git\n",
        "\n",
        "import collections\n",
        "\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rlax\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "\n",
        "# Types \u0026 utils\n",
        "from disco_rl import types\n",
        "from disco_rl import utils\n",
        "\n",
        "# Environments\n",
        "from disco_rl.environments import base as base_env\n",
        "from disco_rl.environments import jittable_envs\n",
        "\n",
        "# Learning\n",
        "from disco_rl import agent as agent_lib\n",
        "\n",
        "axis_name = 'i'  # for parallelisation"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "hinBYQVJqlm0"
      },
      "cell_type": "code",
      "source": [
        "# @title Download and unpack `Disco103` weights.\n",
        "\n",
        "def unflatten_params(flat_params: chex.ArrayTree) -\u003e chex.ArrayTree:\n",
        "  params = {}\n",
        "  for key_wb in flat_params:\n",
        "    key = '/'.join(key_wb.split('/')[:-1])\n",
        "    params[key] = {\n",
        "        'b': flat_params[f'{key}/b'],\n",
        "        'w': flat_params[f'{key}/w'],\n",
        "    }\n",
        "  return params\n",
        "\n",
        "\n",
        "disco_103_fname = 'disco_103.npz'\n",
        "disco_103_url = f\"https://raw.githubusercontent.com/google-deepmind/disco_rl/main/update_rules/weights/{disco_103_fname}\"\n",
        "!wget $disco_103_url\n",
        "\n",
        "with open(f'/content/{disco_103_fname}', 'rb') as file:\n",
        "  disco_103_params = unflatten_params(np.load(file))\n",
        "\n",
        "print(f'Loaded {len(disco_103_params) * 2} parameter tensors for Disco103.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "QhHP6mbdE9Rx"
      },
      "cell_type": "code",
      "source": [
        "# @title Instantiate a simple MLP agent.\n",
        "\n",
        "\n",
        "def get_env(batch_size: int) -\u003e base_env.Environment:\n",
        "  return jittable_envs.CatchJittableEnvironment(\n",
        "      batch_size=batch_size, env_settings=jittable_envs.get_config_catch()\n",
        "  )\n",
        "\n",
        "\n",
        "# Create a dummy environment.\n",
        "env = get_env(batch_size=1)\n",
        "\n",
        "# Create settings for an agent.\n",
        "agent_settings = agent_lib.get_settings_disco()\n",
        "agent_settings.net_settings.name = 'mlp'\n",
        "agent_settings.net_settings.net_args = dict(\n",
        "    dense=(512, 512),\n",
        "    model_arch_name='lstm',\n",
        "    head_w_init_std=1e-2,\n",
        "    model_kwargs=dict(\n",
        "        head_mlp_hiddens=(128,),\n",
        "        lstm_size=128,\n",
        "    ),\n",
        ")\n",
        "agent_settings.learning_rate = 1e-2\n",
        "\n",
        "# Create the agent.\n",
        "agent = agent_lib.Agent(\n",
        "    agent_settings=agent_settings,\n",
        "    single_observation_spec=env.single_observation_spec(),\n",
        "    single_action_spec=env.single_action_spec(),\n",
        "    batch_axis_name=axis_name,\n",
        ")\n",
        "\n",
        "# Ensure that the agent's update rule's parameters have the same specs.\n",
        "random_update_rule_params, _ = agent.update_rule.init_params(\n",
        "    jax.random.PRNGKey(0)\n",
        ")\n",
        "if agent_settings.update_rule_name == 'disco':\n",
        "  chex.assert_trees_all_equal_shapes_and_dtypes(\n",
        "      random_update_rule_params, disco_103_params\n",
        "  )\n",
        "  print('Update rule parameters have the same specs.')\n",
        "else:\n",
        "  print('Not using a discovered rule, skipping check.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "XZnd20Cph9Sl"
      },
      "cell_type": "code",
      "source": [
        "# @title Helper functions for interacting with environments.\n",
        "def unroll_cpu_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "    devices,\n",
        "):\n",
        "  \"\"\"Unrolls the policy for a CPU environments.\"\"\"\n",
        "  actor_timesteps = []\n",
        "  for _ in range(rollout_len):\n",
        "    rng, step_rng = jax.random.split(rng)\n",
        "    step_rng = jax.random.split(step_rng, len(devices))\n",
        "    ts = utils.shard_across_devices(ts, devices)\n",
        "\n",
        "    actor_timestep, actor_state = actor_step_fn(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    actions = utils.gather_from_devices(actor_timestep.actions)\n",
        "    env_state, ts = env.step(env_state, actions)\n",
        "\n",
        "    actor_timesteps.append(actor_timestep)\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(\n",
        "      utils.tree_stack(actor_timesteps, axis=1)\n",
        "  )\n",
        "  return actor_rollout, actor_state, ts, env_state\n",
        "\n",
        "\n",
        "def unroll_jittable_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "    devices,\n",
        "):\n",
        "  \"\"\"Unrolls the policy for a jittable environment.\"\"\"\n",
        "  del actor_step_fn, devices\n",
        "\n",
        "  def _single_step(carry, step_rng):\n",
        "    env_state, ts, actor_state = carry\n",
        "    actor_timestep, actor_state = agent.actor_step(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    env_state, ts = env.step(env_state, actor_timestep.actions)\n",
        "    return (env_state, ts, actor_state), actor_timestep\n",
        "\n",
        "  (env_state, ts, actor_state), actor_rollout = jax.lax.scan(\n",
        "      _single_step,\n",
        "      (env_state, ts, actor_state),\n",
        "      jax.random.split(rng, rollout_len),\n",
        "  )\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(actor_rollout)\n",
        "  return actor_rollout, actor_state, ts, env_state\n",
        "\n",
        "\n",
        "def accumulate_rewards(acc_rewards, x):\n",
        "  rewards, discounts = x\n",
        "\n",
        "  def _step_fn(acc_rewards, x):\n",
        "    rewards, discounts = x\n",
        "    acc_rewards += rewards\n",
        "    return acc_rewards * discounts, acc_rewards\n",
        "\n",
        "  return jax.lax.scan(_step_fn, acc_rewards, (rewards, discounts))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "lJ4HfawLSMWH"
      },
      "cell_type": "code",
      "source": [
        "# @title A simple Replay buffer.\n",
        "\n",
        "\n",
        "class SimpleReplayBuffer:\n",
        "  \"\"\"A simple FIFO replay buffer for JAX arrays.\"\"\"\n",
        "\n",
        "  def __init__(self, capacity: int, seed: int):\n",
        "    \"\"\"Initializes the buffer.\"\"\"\n",
        "    self.buffer = collections.deque(maxlen=capacity)\n",
        "    self.capacity = capacity\n",
        "    self.np_rng = np.random.default_rng(seed)\n",
        "\n",
        "  def add(self, rollout: types.ActorRollout) -\u003e None:\n",
        "    \"\"\"Appends a batch of trajectories to the buffer.\"\"\"\n",
        "    rollout = jax.device_get(rollout)\n",
        "    # split_tree = split_tree_on_dim(rollout, 2)\n",
        "    split_tree = rlax.tree_split_leaves(rollout, axis=2)  # across batch dim\n",
        "    self.buffer.extend(split_tree)\n",
        "\n",
        "  def sample(self, batch_size: int) -\u003e types.ActorRollout | None:\n",
        "    \"\"\"Samples a batch of trajectories from the buffer.\"\"\"\n",
        "    buffer_size = len(self.buffer)\n",
        "    if buffer_size == 0:\n",
        "      print(\"Warning: Trying to sample from an empty buffer.\")\n",
        "      return None\n",
        "\n",
        "    indices = self.np_rng.integers(buffer_size, size=batch_size)\n",
        "    batched_samples = utils.tree_stack(\n",
        "        [self.buffer[i] for i in indices], axis=2\n",
        "    )\n",
        "    return batched_samples\n",
        "\n",
        "  def __len__(self) -\u003e int:\n",
        "    \"\"\"Returns the current number of transitions in the buffer.\"\"\"\n",
        "    return len(self.buffer)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "lmpFAJ84HUjS"
      },
      "cell_type": "code",
      "source": [
        "# @title Training loop\n",
        "\n",
        "num_steps = 1000\n",
        "batch_size = 64\n",
        "rollout_len = 29\n",
        "rng_key = jax.random.PRNGKey(0)\n",
        "\n",
        "replay_ratio = 32\n",
        "buffer = SimpleReplayBuffer(capacity=1024, seed=17)\n",
        "min_buffer_size = batch_size\n",
        "\n",
        "num_envs = batch_size // replay_ratio\n",
        "devices = tuple(jax.devices()[:num_envs])\n",
        "env = get_env(num_envs)\n",
        "\n",
        "# Init states.\n",
        "env_state, ts = env.reset(rng_key)\n",
        "acc_rewards = jnp.zeros((num_envs,))\n",
        "learner_state = agent.initial_learner_state(rng_key)\n",
        "actor_state = agent.initial_actor_state(rng_key)\n",
        "update_rule_params = disco_103_params\n",
        "\n",
        "# Parallelise training across all available devices.\n",
        "actor_step_fn = jax.pmap(agent.actor_step, axis_name, devices=devices)\n",
        "learner_step_fn = jax.pmap(\n",
        "    agent.learner_step,\n",
        "    axis_name=axis_name,\n",
        "    devices=devices,\n",
        "    static_broadcasted_argnums=(5,),\n",
        ")\n",
        "jitted_unroll_actor = jax.pmap(\n",
        "    unroll_jittable_actor,\n",
        "    axis_name=axis_name,\n",
        "    devices=devices,\n",
        "    static_broadcasted_argnums=(5, 6, 7, 8),\n",
        ")\n",
        "jitted_accumulate_rewards = jax.pmap(\n",
        "    accumulate_rewards,\n",
        "    axis_name=axis_name,\n",
        "    devices=devices,\n",
        ")\n",
        "\n",
        "# Replicate onto the devices.\n",
        "learner_state = jax.device_put_replicated(learner_state, devices)\n",
        "actor_state = jax.device_put_replicated(actor_state, devices)\n",
        "update_rule_params = jax.device_put_replicated(update_rule_params, devices)\n",
        "\n",
        "is_jittable_actor = isinstance(\n",
        "    env, jittable_envs.batched_jittable_env.BatchedJittableEnvironment\n",
        ")\n",
        "\n",
        "if is_jittable_actor:\n",
        "  unroll_actor = jitted_unroll_actor\n",
        "  acc_rewards_fn = jitted_accumulate_rewards\n",
        "  acc_rewards = jnp.zeros((num_envs,))\n",
        "  (env_state, ts, acc_rewards) = utils.shard_across_devices(\n",
        "      (env_state, ts, acc_rewards), devices\n",
        "  )\n",
        "else:\n",
        "  unroll_actor = unroll_cpu_actor\n",
        "  acc_rewards_fn = accumulate_rewards\n",
        "\n",
        "# Buffers.\n",
        "all_metrics = []\n",
        "all_rewards = []\n",
        "all_discounts = []\n",
        "all_steps = []\n",
        "all_returns = []\n",
        "total_steps = 0\n",
        "\n",
        "# Run the loop.\n",
        "for step in tqdm.tqdm(range(num_steps)):\n",
        "  rng_key, rng_actor, rng_learner = jax.random.split(rng_key, 3)\n",
        "\n",
        "  if is_jittable_actor:\n",
        "    rng_actor = jax.random.split(rng_actor, len(devices))\n",
        "\n",
        "  # Generate new trajectories and add them to the buffer.\n",
        "  actor_rollout, actor_state, ts, env_state = unroll_actor(\n",
        "      learner_state.params,\n",
        "      actor_state,\n",
        "      ts,\n",
        "      env_state,\n",
        "      rng_actor,\n",
        "      env,\n",
        "      rollout_len,\n",
        "      actor_step_fn,\n",
        "      devices,\n",
        "  )\n",
        "  buffer.add(actor_rollout)\n",
        "\n",
        "  # Accumulate statistics.\n",
        "  total_steps += np.prod(actor_rollout.rewards.shape)\n",
        "  acc_rewards, returns = acc_rewards_fn(\n",
        "      acc_rewards,\n",
        "      (actor_rollout.rewards, actor_rollout.discounts),\n",
        "  )\n",
        "  all_steps.append(total_steps)\n",
        "  all_rewards.append(jax.device_get(actor_rollout.rewards))\n",
        "  all_discounts.append(jax.device_get(actor_rollout.discounts))\n",
        "  all_returns.append(jax.device_get(returns))\n",
        "\n",
        "  # Update agent's parameters on the samples from the buffer.\n",
        "  rng_learner = jax.random.split(rng_learner, len(devices))\n",
        "  if len(buffer) \u003e= min_buffer_size:\n",
        "    learner_rollout = buffer.sample(batch_size)\n",
        "    learner_state, _, metrics = learner_step_fn(\n",
        "        rng_learner,\n",
        "        learner_rollout,\n",
        "        learner_state,\n",
        "        actor_state,\n",
        "        update_rule_params,\n",
        "        False,  # is_meta_training\n",
        "    )\n",
        "    all_metrics.append(jax.device_get(metrics))\n",
        "\n",
        "# Collect all logs and statistics.\n",
        "all_metrics, all_rewards, all_discounts, all_returns = (\n",
        "    utils.gather_from_devices(\n",
        "        (all_metrics, all_rewards, all_discounts, all_returns)\n",
        "    )\n",
        ")\n",
        "(all_metrics,) = jax.tree.map(lambda x: x.mean(0), (all_metrics,))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "80kt3EUbIps0"
      },
      "cell_type": "code",
      "source": [
        "# @title Process logs\n",
        "all_returns = np.array(all_returns)\n",
        "all_discounts = np.array(all_discounts)\n",
        "all_steps = np.array(all_steps)\n",
        "total_returns = (all_returns * (1 - all_discounts)).sum(axis=(1, 2))\n",
        "total_episodes = (1 - all_discounts).sum(axis=(1, 2))\n",
        "avg_returns = total_returns / total_episodes\n",
        "\n",
        "padded_metrics = {}\n",
        "pad_width = len(all_steps) - len(all_metrics)\n",
        "for key in all_metrics[0].keys():\n",
        "  values = np.array([m[key] for m in all_metrics])\n",
        "  padded_metrics[key] = np.pad(values, (pad_width, 0), constant_values=np.nan)\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    dict(\n",
        "        steps=all_steps,\n",
        "        avg_returns=avg_returns,\n",
        "        **padded_metrics,\n",
        "    )\n",
        ")\n",
        "\n",
        "df['name'] = agent_settings.update_rule_name"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "se4UE1W93uRt"
      },
      "cell_type": "code",
      "source": [
        "sns.lineplot(data=df, x='steps', y='avg_returns')"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
