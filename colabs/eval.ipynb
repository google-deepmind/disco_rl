{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oxP_07h63p7"
      },
      "source": [
        "**NOTE:** When running in public colab, this cell will prompt you to upload the archive with the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JUG8YPRjnJt"
      },
      "outputs": [],
      "source": [
        "# @title Environment setup\n",
        "\n",
        "\n",
        "!pip install git+https://github.com/google-deepmind/disco_rl.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_QQRMPQaLip"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "\n",
        "loaded_update_rule_params = None\n",
        "rng_key = jax.random.PRNGKey(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAX6bxL9_SrC"
      },
      "outputs": [],
      "source": [
        "# Types \u0026 utils\n",
        "from disco_rl import types\n",
        "from disco_rl import utils\n",
        "\n",
        "# Environments\n",
        "from disco_rl.environments import jittable_envs\n",
        "\n",
        "# Learning\n",
        "from disco_rl import agent as agent_lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FQ-yqkX_SrC"
      },
      "outputs": [],
      "source": [
        "eta_path = ''\n",
        "if loaded_update_rule_params is None:\n",
        "  loaded_update_rule_params = {}\n",
        "  with open(eta_path, 'rb') as file:\n",
        "    ur_params_wb = dict(np.load(file))\n",
        "    for key_wb in ur_params_wb:\n",
        "      key = key_wb[:-2]\n",
        "      loaded_update_rule_params[key] = {\n",
        "          'b': ur_params_wb[f'{key}/b'],\n",
        "          'w': ur_params_wb[f'{key}/w'],\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhHP6mbdE9Rx"
      },
      "outputs": [],
      "source": [
        "agent_settings = agent_lib.get_settings_disco()\n",
        "\n",
        "\n",
        "def get_env(batch_size):\n",
        "  # return catch.CatchEnvironment(batch_size=batch_size, env_settings=catch.get_config())\n",
        "  return jittable_envs.CatchJittableEnvironment(\n",
        "      batch_size=batch_size, env_settings=jittable_envs.get_config_catch()\n",
        "  )\n",
        "\n",
        "\n",
        "env = get_env(batch_size=1)\n",
        "\n",
        "agent_settings.net_settings.name = 'mlp'\n",
        "agent_settings.net_settings.net_args = dict(\n",
        "    dense=(512, 512),\n",
        "    model_arch_name='lstm',\n",
        "    head_w_init_std=1e-2,\n",
        "    model_kwargs=dict(\n",
        "        head_mlp_hiddens=(128,),\n",
        "        lstm_size=128,\n",
        "    ),\n",
        ")\n",
        "agent_settings.learning_rate = 1e-2\n",
        "agent_settings.end_learning_rate = agent_settings.learning_rate\n",
        "agent_settings.weight_decay = 1e-3\n",
        "\n",
        "agent = agent_lib.Agent(\n",
        "    agent_settings=agent_settings,\n",
        "    single_observation_spec=env.single_observation_spec(),\n",
        "    single_action_spec=env.single_action_spec(),\n",
        "    batch_axis_name='i',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWWHy11qTaCT"
      },
      "outputs": [],
      "source": [
        "random_update_rule_params, _ = agent.update_rule.init_params(rng_key)\n",
        "\n",
        "if agent_settings.update_rule_name == 'disco':\n",
        "  chex.assert_trees_all_equal_shapes_and_dtypes(\n",
        "      random_update_rule_params, loaded_update_rule_params\n",
        "  )\n",
        "else:\n",
        "  print('Not using a discovered rule.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZnd20Cph9Sl"
      },
      "outputs": [],
      "source": [
        "def unroll_cpu_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "    devices,\n",
        "):\n",
        "  actor_timesteps = []\n",
        "  for _ in range(rollout_len):\n",
        "    rng, step_rng = jax.random.split(rng)\n",
        "    step_rng = jax.random.split(step_rng, len(devices))\n",
        "    ts = utils.shard_across_devices(ts, devices)\n",
        "\n",
        "    actor_timestep, actor_state = actor_step_fn(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    actions = utils.gather_from_devices(actor_timestep.actions)\n",
        "    env_state, ts = env.step(env_state, actions)\n",
        "\n",
        "    actor_timesteps.append(actor_timestep)\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(\n",
        "      utils.tree_stack(actor_timesteps, axis=1)\n",
        "  )\n",
        "  return actor_rollout, actor_state, ts, env_state\n",
        "\n",
        "\n",
        "def unroll_jittable_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "    devices,\n",
        "):\n",
        "  del actor_step_fn, devices\n",
        "  actor_step_fn = agent.actor_step\n",
        "\n",
        "  def _single_step(carry, step_rng):\n",
        "    env_state, ts, actor_state = carry\n",
        "    actor_timestep, actor_state = actor_step_fn(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    env_state, ts = env.step(env_state, actor_timestep.actions)\n",
        "    return (env_state, ts, actor_state), actor_timestep\n",
        "\n",
        "  (env_state, ts, actor_state), actor_rollout = jax.lax.scan(\n",
        "      _single_step,\n",
        "      (env_state, ts, actor_state),\n",
        "      jax.random.split(rng, rollout_len),\n",
        "  )\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(actor_rollout)\n",
        "  return actor_rollout, actor_state, ts, env_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ4HfawLSMWH"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "def split_tree_on_dim(tree, axis: int):\n",
        "  \"\"\"Splits all array leaves of a PyTree along a given axis into singletons.\"\"\"\n",
        "  leaves = jax.tree_util.tree_leaves(tree)\n",
        "  split_size = -1\n",
        "\n",
        "  for leaf in leaves:\n",
        "    if isinstance(leaf, (jnp.ndarray, np.ndarray)) and leaf.ndim \u003e axis:\n",
        "      current_dim_size = leaf.shape[axis]\n",
        "      if split_size == -1:\n",
        "        split_size = current_dim_size\n",
        "      elif split_size != current_dim_size:\n",
        "        raise ValueError(\n",
        "            f\"Inconsistent dimension sizes found for axis {axis}. \"\n",
        "            f\"Expected {split_size}, but found leaf with shape {leaf.shape} \"\n",
        "            f\"(size {current_dim_size}).\"\n",
        "        )\n",
        "\n",
        "  output_trees = []\n",
        "  for i in range(split_size):\n",
        "\n",
        "    def slice_leaf(leaf):\n",
        "      if (\n",
        "          isinstance(leaf, (jnp.ndarray, np.ndarray))\n",
        "          and leaf.ndim \u003e axis\n",
        "          and leaf.shape[axis] == split_size\n",
        "      ):\n",
        "        return jnp.expand_dims(jnp.take(leaf, i, axis=axis), axis)\n",
        "      else:\n",
        "        return leaf\n",
        "\n",
        "    sliced_tree = jax.tree.map(slice_leaf, tree)\n",
        "    output_trees.append(sliced_tree)\n",
        "  return output_trees\n",
        "\n",
        "\n",
        "class SimpleReplayBuffer:\n",
        "  \"\"\"A simple FIFO replay buffer for JAX arrays.\"\"\"\n",
        "\n",
        "  def __init__(self, capacity: int, seed: int):\n",
        "    self.buffer = collections.deque(maxlen=capacity)\n",
        "    self.capacity = capacity\n",
        "    self.np_rng = np.random.default_rng(seed)\n",
        "\n",
        "  def add(self, rollout: types.ActorRollout):\n",
        "    rollout = jax.device_get(rollout)\n",
        "    split_tree = split_tree_on_dim(rollout, 2)\n",
        "    self.buffer.extend(split_tree)\n",
        "\n",
        "  def sample(self, batch_size: int) -\u003e types.ActorRollout | None:\n",
        "    buffer_size = len(self.buffer)\n",
        "    if buffer_size == 0:\n",
        "      print(\"Warning: Trying to sample from an empty buffer.\")\n",
        "      return None\n",
        "\n",
        "    indices = self.np_rng.integers(buffer_size, size=batch_size)\n",
        "    samples = [self.buffer[i] for i in indices]\n",
        "    batched_samples = jax.tree.map(\n",
        "        lambda *x: np.concatenate(x, axis=2), *samples\n",
        "    )\n",
        "    return batched_samples\n",
        "\n",
        "  def __len__(self) -\u003e int:\n",
        "    \"\"\"Returns the current number of transitions in the buffer.\"\"\"\n",
        "    return len(self.buffer)\n",
        "\n",
        "\n",
        "def accumulate_rewards_scan_fn(carry, x):\n",
        "  acc_rewards = carry\n",
        "  rewards, discounts = x\n",
        "  acc_rewards += rewards\n",
        "  return acc_rewards * discounts, acc_rewards\n",
        "\n",
        "\n",
        "def accumulate_rewards(acc_rewards, x):\n",
        "  rewards, discounts = x\n",
        "  return jax.lax.scan(\n",
        "      accumulate_rewards_scan_fn, acc_rewards, (rewards, discounts)\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmpFAJ84HUjS"
      },
      "outputs": [],
      "source": [
        "replay_ratio = 32\n",
        "\n",
        "rollout_len = 29\n",
        "num_steps = 1000\n",
        "batch_size = 64\n",
        "min_buffer_size = batch_size\n",
        "\n",
        "rng_key = jax.random.PRNGKey(0)\n",
        "\n",
        "num_envs = batch_size // replay_ratio\n",
        "devices = tuple(jax.devices()[:num_envs])\n",
        "env = get_env(num_envs)\n",
        "\n",
        "actor_step_fn = jax.pmap(agent.actor_step, 'i', devices=devices)\n",
        "learner_step_fn = jax.pmap(agent.learner_step, 'i', devices=devices)\n",
        "unroll_jittable_actor = jax.pmap(\n",
        "    unroll_jittable_actor,\n",
        "    axis_name='i',\n",
        "    devices=devices,\n",
        "    static_broadcasted_argnums=(5, 6, 7, 8),\n",
        ")\n",
        "jittable_accumulate_rewards = jax.pmap(\n",
        "    accumulate_rewards,\n",
        "    axis_name='i',\n",
        "    devices=devices,\n",
        ")\n",
        "\n",
        "learner_state = agent.initial_learner_state(rng_key)\n",
        "actor_state = agent.initial_actor_state(rng_key)\n",
        "\n",
        "update_rule_params = loaded_update_rule_params\n",
        "update_rule_params = jax.device_put_replicated(update_rule_params, devices)\n",
        "\n",
        "env_state, ts = env.reset(rng_key)\n",
        "acc_rewards = jnp.zeros((num_envs,))\n",
        "is_jittable_actor = isinstance(\n",
        "    env, jittable_envs.batched_jittable_env.BatchedJittableEnvironment\n",
        ")\n",
        "\n",
        "learner_state = jax.device_put_replicated(learner_state, devices)\n",
        "actor_state = jax.device_put_replicated(actor_state, devices)\n",
        "\n",
        "if is_jittable_actor:\n",
        "  unroll_actor = unroll_jittable_actor\n",
        "  acc_rewards_fn = jittable_accumulate_rewards\n",
        "  acc_rewards = jnp.zeros((num_envs,))\n",
        "  (env_state, ts, acc_rewards) = utils.shard_across_devices(\n",
        "      (env_state, ts, acc_rewards), devices\n",
        "  )\n",
        "else:\n",
        "  unroll_actor = unroll_cpu_actor\n",
        "  acc_rewards_fn = accumulate_rewards\n",
        "\n",
        "buffer = SimpleReplayBuffer(capacity=1024, seed=17)\n",
        "\n",
        "all_metrics = []\n",
        "all_rewards = []\n",
        "all_discounts = []\n",
        "all_steps = []\n",
        "all_returns = []\n",
        "total_steps = 0\n",
        "\n",
        "for step in tqdm.tqdm(range(num_steps)):\n",
        "  rng_key, rng_actor, rng_learner = jax.random.split(rng_key, 3)\n",
        "\n",
        "  if is_jittable_actor:\n",
        "    rng_actor = jax.random.split(rng_actor, len(devices))\n",
        "\n",
        "  actor_rollout, actor_state, ts, env_state = unroll_actor(\n",
        "      learner_state.params,\n",
        "      actor_state,\n",
        "      ts,\n",
        "      env_state,\n",
        "      rng_actor,\n",
        "      env,\n",
        "      rollout_len,\n",
        "      actor_step_fn,\n",
        "      devices,\n",
        "  )\n",
        "  buffer.add(actor_rollout)\n",
        "\n",
        "  total_steps += np.prod(actor_rollout.rewards.shape)\n",
        "  acc_rewards, returns = acc_rewards_fn(\n",
        "      acc_rewards,\n",
        "      (actor_rollout.rewards, actor_rollout.discounts),\n",
        "  )\n",
        "\n",
        "  all_steps.append(total_steps)\n",
        "  all_rewards.append(jax.device_get(actor_rollout.rewards))\n",
        "  all_discounts.append(jax.device_get(actor_rollout.discounts))\n",
        "  all_returns.append(jax.device_get(returns))\n",
        "\n",
        "  rng_learner = jax.random.split(rng_learner, len(devices))\n",
        "\n",
        "  if len(buffer) \u003e= min_buffer_size:\n",
        "    learner_batch = buffer.sample(batch_size)\n",
        "    learner_state, _, metrics = learner_step_fn(\n",
        "        rng=rng_learner,\n",
        "        rollout=learner_batch,\n",
        "        learner_state=learner_state,\n",
        "        agent_net_state=actor_state,\n",
        "        update_rule_params=update_rule_params,\n",
        "    )\n",
        "    all_metrics.append(jax.device_get(metrics))\n",
        "\n",
        "\n",
        "all_metrics, all_rewards, all_discounts, all_returns = (\n",
        "    utils.gather_from_devices(\n",
        "        (all_metrics, all_rewards, all_discounts, all_returns)\n",
        "    )\n",
        ")\n",
        "(all_metrics,) = jax.tree.map(lambda x: x.mean(0), (all_metrics,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80kt3EUbIps0"
      },
      "outputs": [],
      "source": [
        "all_returns = np.array(all_returns)\n",
        "all_discounts = np.array(all_discounts)\n",
        "all_steps = np.array(all_steps)\n",
        "total_returns = (all_returns * (1 - all_discounts)).sum(axis=(1, 2))\n",
        "total_episodes = (1 - all_discounts).sum(axis=(1, 2))\n",
        "avg_returns = total_returns / total_episodes\n",
        "\n",
        "padded_metrics = {}\n",
        "pad_width = len(all_steps) - len(all_metrics)\n",
        "for key in all_metrics[0].keys():\n",
        "  values = np.array([m[key] for m in all_metrics])\n",
        "  padded_metrics[key] = np.pad(values, (pad_width, 0), constant_values=np.nan)\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    dict(\n",
        "        steps=all_steps,\n",
        "        avg_returns=avg_returns,\n",
        "        **padded_metrics,\n",
        "    )\n",
        ")\n",
        "\n",
        "df['name'] = agent_settings.update_rule_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UhpLpwcSI4a"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(data=df, x='steps', y='avg_returns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se4UE1W93uRt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
